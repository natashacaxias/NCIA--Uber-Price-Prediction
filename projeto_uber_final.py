# -*- coding: utf-8 -*-
"""Projeto_UBer_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQEo0lmlk1WjZCqaOqFJcREqnJjBeqjM

### **Projeto - Previsão de preço UBER | NCIA**

**Este arquivo é de propriedade da FPF e seu uso é permitido apenas por ela e seus parceiros autorizados.**

**Desenvolvido pelo grupo A para a disciplina de Machine Learning**
**Professor** Antonio

---

### Membros da Equipe

-   **Aluno(a) 1:** [Gustavo B. Lima] - [252314]
-   **Aluno(a) 2:** [Alessandra L. Bentes] - [221452]
-   **Aluno(a) 3:** [Natasha A. Caxias] - [221778]
-   **Aluno(a) 4:** [Tedy Prist] - [221743]
-   **Aluno(a) 5:** [Kevyn Goldim] - [252323]

---
# Projeto - Previsões de Preços no Aplicativo de Corrida UBER

Este notebook demonstra o pipeline completo de Machine Learning desenvolvido para o **Projeto NCIA - Previsão de Preços em Aplicativos de Corrida**.

O objetivo é analisar um conjunto de dados de viagens na região de **Boston, MA (EUA)**, entender os fatores que influenciam o preço final de uma corrida (distância, clima, tipo de carro, demanda, etc.), e construir um modelo capaz de prever o `price` com alta precisão.

# Parte 1 - Análise do Dataset

## 1.0 Carregamento dos Dados
O conjunto de dados (**rideshare_kaggle.csv**) foi obtido do Kaggle. A função `load_kaggle_data()` gerencia o download e o carregamento para garantir que o *notebook* seja reprodutível:

1.  **Chama a API** do Kaggle para localizar o *dataset* `uber-and-lyft-dataset-boston-ma`.
2.  **Baixa** o arquivo **rideshare_kaggle.csv** para o ambiente local (se ainda não estiver presente).
3.  **Carrega** e retorna os dados em um DataFrame do Pandas (`df`).

> **Nota**: O arquivo CSV contém mais de 690 mil registros de viagens (Uber e Lyft), com 57 atributos, incluindo dados geográficos, temporais e de condições climáticas.
"""

from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
import matplotlib.pyplot as plt
import kagglehub
import seaborn as  sns
import numpy as np


def load_kaggle_data(dataset: str, filename: str):
    path = kagglehub.dataset_download(dataset)
    file_path = Path(path) / filename

    if not file_path.is_file():
        raise FileNotFoundError(f"Arquivo {filename} não encontrado em {path}")

    return pd.read_csv(file_path)

"""

def load_kaggle_data(dataset: str, filename: str):
    # Versão local — não usa kagglehub
    local_path = Path("data") / filename
    if not local_path.exists():
        raise FileNotFoundError(f"Arquivo {filename} não encontrado em {local_path}")
    return pd.read_csv(local_path)
"""
df = load_kaggle_data("brllrb/uber-and-lyft-dataset-boston-ma", "rideshare_kaggle.csv")

"""## 2. Exploração Inicial dos Dados (EDA)

Nosso conjunto de dados possui mais de **690 mil registros** de viagens e **57 atributos**. Para manter o notebook conciso, organizaremos as variáveis em grupos temáticos:

### Atributos Chave e Grupos de Variáveis

| Grupo de Atributos | Descrição | Variáveis Chave para Previsão |
| :--- | :--- | :--- |
| **Variável Alvo** | O valor que queremos prever. | `price` |
| **Viagem** | Informações da corrida em si. | `distance`, `cab_type`, `name`, `source`, `destination` |
| **Tempo** | Dados temporais e sazonais. | `datetime`, `hour`, `day`, `month` |
| **Clima** | Condições climáticas na hora da viagem. | `apparentTemperature`, `short_summary`, `precipIntensity` |
| **Demanda** | Multiplicador de preço por alta demanda. | `surge_multiplier` |

> **Nota de Análise:** Muitas colunas no dataset original são redundantes (ex: diferentes representações de tempo ou clima). O foco inicial será nas **Variáveis Chave** listadas acima e na limpeza das variáveis que apresentam alto número de valores nulos (missing values).
"""

df.head()

print(f"Quantidade de Instâncias e Atributos: {df.shape}")
df.info()

"""### 2.3. Análise da Estrutura dos Dados

A saída do método `info()` nos fornece um panorama detalhado da estrutura do DataFrame:

1.  **Tamanho do Dataset:**
    * **Instâncias (Viagens):** O *dataset* contém **693.071** registros.
    * **Atributos (Características):** Há um total de **57** colunas.

2.  **Tipos de Dados:**
    * A maioria é composta por variáveis numéricas: **29 `float64`** e **17 `int64`**.
    * Existem **11 atributos categóricos** (tipo `object`), que incluem dados como `id`, `source`, `destination`, `cab_type`, `name` (tipo de serviço) e `short_summary` (clima).

3.  **Valores Ausentes (Missing Values):**
    * Identificamos um volume expressivo de valores ausentes na **Variável Alvo** do projeto: **`price`**.
    * De um total de 693.071 registros, **55.095 linhas** não possuem um valor de preço, o que representa aproximadamente **7.95%** do *dataset*.
    * Os demais 56 atributos estão **completos** (693.071 *non-null*).

***

### Próximos Passos Prioritários

Esta análise inicial estabelece os requisitos cruciais para a fase de Pré-Processamento:

-   **Tratamento da Variável Alvo (`price`):** Como o `price` é a variável que o modelo deve prever, as linhas com valores nulos **devem ser removidas** do conjunto de treino/teste.
-   **Engenharia de Recursos (Feature Engineering):** É necessário transformar as colunas temporais (`timestamp`, `datetime`) em recursos úteis e codificar as 11 variáveis categóricas (`object`) para que possam ser utilizadas pelos modelos.
-   **Dimensionalidade:** O alto número de colunas (57) indica que a **Seleção e Redução de Features** será necessária para otimizar o modelo e evitar *overfitting*.
"""

df.describe()

"""### 2.4. Análise Estatística das Variáveis Numéricas (`df.describe()`)

A tabela de estatísticas descritivas confirma o comportamento e a distribuição dos dados.

#### 1. Variável Alvo: `price`

| Estatística | Valor | Implicação para o Modelo |
| :--- | :--- | :--- |
| **Média** | \$15.79 | A média é significativamente maior que a Mediana, indicando assimetria. |
| **Mediana (50%)** | \$12.50 | Metade das corridas custa menos de \$12.50. |
| **Máximo** | \$89.50 | O valor máximo, muito distante do **75º Percentil (\$21.50)**, sugere a presença de **outliers** (corridas de alto valor) que podem impactar modelos lineares. |
| **Desvio Padrão ($\sigma$)** | \$8.56 | O alto desvio padrão (próximo à média) confirma a grande dispersão dos preços. |

> **Conclusão:** A distribuição do `price` é **assimétrica à direita** (possui cauda longa). Isso sugere que a aplicação de uma **transformação logarítmica** (ou similar) na variável alvo pode ser necessária para melhorar o desempenho de alguns modelos de regressão.

#### 2. Características da Viagem

* **`distance` (Distância):** A distância média percorrida é de **2.19 milhas**. O 75º percentil é de apenas 2.84 milhas, indicando que a grande maioria das viagens em Boston é de curta ou média distância.
* **`duration` (Duração - Feature Criada):** A duração média é de **6.57 minutos**. O valor segue a distribuição de distância, o que é esperado, visto que esta *feature* foi criada a partir de `distance`.
* **`surge_multiplier` (Multiplicador de Demanda):** A média de **1.03** (com desvio padrão muito baixo) sugere que a grande maioria das corridas não teve preço de demanda alto. O *surge* ativou-se em poucas situações no período de coleta.

#### 3. Sazonalidade e Clima

* **Sazonalidade (`month`):** O `min` e `max` de **11 e 12** confirmam que o *dataset* cobre apenas o período de **Novembro e Dezembro de 2018**, o que é crucial para limitar a generalização do modelo a outros períodos do ano.
* **Clima (`apparentTemperature`):** A temperatura aparente média é de **35.85 graus Fahrenheit (aproximadamente $2^\circ C$)**. O `min` de $12.13^\circ F$ (aproximadamente $-11^\circ C$) confirma que a coleta foi realizada em um **período frio**, o que pode correlacionar o preço com a indisponibilidade ou alta demanda causada pelo mau tempo.
* **Precipitação:** A probabilidade máxima de precipitação (`precipProbability`) é **1.0 (100%)**, apesar de a média ser baixa (0.146). Isso é positivo, pois garante que o modelo tem exemplos de corridas realizadas sob condições climáticas adversas.

# 3. Pré-Processamento

Com base nas conclusões da análise exploratória (Seção 2 parte 1), o passo crítico agora é padronizar os dados e extrair informações valiosas. Utilizaremos a função `limparDados` para encapsular todo o *pipeline* de transformação e garantir a reprodutibilidade.

A função `limparDados` irá realizar as seguintes operações em sequência:

## 3.1. Pipeline de Limpeza e Transformação

1.  **Tratamento Temporal:** Converter as *strings* `datetime` e `timestamp` para o formato nativo `datetime` do Pandas, permitindo extrair recursos como hora (`hour`), dia (`day`), e mês (`month`).
2.  **Criação de Feature (Duration):** Criar a variável `duration` (duração da viagem) com base na fórmula `distance / 20 mph`, uma **hipótese simplificada** para o tempo de percurso.
3.  **Seleção de Features:** Manter apenas as 20 colunas consideradas mais relevantes para o modelo de previsão (Viagem, Clima, Tempo e Alvo).
4.  **Codificação de Variáveis Categóricas:** Aplicar **Label Encoding** nas variáveis textuais como `source`, `cab_type` e `short_summary`. *Nota: Embora o One-Hot Encoding seja ideal para variáveis nominais, o Label Encoding será mantido por enquanto para simplificação inicial.*
5.  **Tratamento de Missing Values:** **Remover** as 55.095 linhas em que a variável alvo (`price`) está nula, preparando o *dataset* para o treinamento do modelo de regressão.
"""

def limparDados(df):
    from sklearn.preprocessing import LabelEncoder

    df = df.copy()  # Prevenir modificar o df original

    df['datetime'] = pd.to_datetime(df['datetime'])
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df['hour'] = df['datetime'].dt.hour
    df['day'] = df['datetime'].dt.day
    df['month'] = df['datetime'].dt.month

    df['duration'] = (df['distance'] / 20) * 60

    column = ['datetime', 'hour', 'day', 'month',
              'source', 'destination', 'cab_type', 'name',
              'price', 'distance', 'surge_multiplier',
              'apparentTemperature', 'short_summary',
              'precipIntensity', 'precipProbability', 'icon','latitude','longitude','duration']
    df = df[[col for col in column if col in df.columns]]
    label_encoder = LabelEncoder()
    df["source_encoded"] = label_encoder.fit_transform(df["source"])
    df["destination_encoded"] = label_encoder.fit_transform(df["destination"])
    df["cab_type_encoded"] = label_encoder.fit_transform(df["cab_type"])
    df["name_encoded"] = label_encoder.fit_transform(df["name"])
    df["short_summary_encoded"] = label_encoder.fit_transform(df["short_summary"])
    df["icon_encoded"] = label_encoder.fit_transform(df["icon"])


    df = df.dropna(subset=["price"])

    return df

df=limparDados(df)

"""## 3.2. Análise da Distribuição de Variáveis Categóricas

Com a seleção das colunas mais importantes concluída, a próxima etapa da EDA é visualizar a distribuição das **variáveis categóricas** (tipo `object`).

Esta análise é fundamental para entender o contexto do nosso projeto de previsão de preços, pois ela nos mostra:
1.  **Demanda Geográfica:** Quais locais (`source` e `destination`) têm o maior volume de corridas.
2.  **Popularidade do Serviço:** Quais tipos de táxi (`cab_type`) e nomes de serviço (`name`) são mais utilizados.
3.  **Contexto Climático:** A frequência das diferentes descrições climáticas (`short_summary`, `icon`).

## 3.3. Visualização de Frequência

A célula abaixo filtra as colunas categóricas e gera um **gráfico de barras** para cada uma, mostrando a contagem total de corridas para cada categoria.

* **Seleção de Subconjunto:** Cria um DataFrame temporário (`df_part`) apenas com as colunas relevantes que foram selecionadas na fase de pré-processamento.
* **Contagem:** Para cada coluna categórica, o método `value_counts()` é utilizado para contar a frequência de cada categoria.
* **Plotagem:** Utiliza o `seaborn.barplot` para plotar essas contagens, permitindo a comparação visual da demanda e distribuição em todo o *dataset*.
"""

cols_to_keep = [
    'datetime', 'hour', 'day', 'month', 'source', 'destination',
    'cab_type', 'name', 'price', 'distance', 'surge_multiplier',
    'apparentTemperature', 'short_summary', 'precipIntensity',
    'precipProbability', 'icon', 'latitude', 'longitude', 'duration',
    'source_encoded', 'destination_encoded', 'cab_type_encoded',
    'name_encoded', 'short_summary_encoded', 'icon_encoded'
]
df_part = df[cols_to_keep].copy()

categorical_cols = df_part.select_dtypes(include=['object']).columns
n_cols = 3
n_rows = (len(categorical_cols) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(categorical_cols):
    ax = axes[i]
    top_values = df_part[col].value_counts()
    sns.barplot(
    y=top_values.index,
    x=top_values.values,
    hue=top_values.index,
    palette='bright',
    ax=ax,
    legend=False
)

    ax.set_title(f'{col}', fontsize=12)
    ax.set_xlabel('Número de corridas', fontsize=10)
    ax.set_ylabel(col, fontsize=10)
    ax.tick_params(axis='x', rotation=45)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## 3.4. Interpretação dos Gráficos
*   cab_type: tipo de serviço (Uber ou Lyft).
*   name: modalidade do serviço.
*   source e destination: locais de origem e destino da corrida.
*   day/night: período do dia.
*   weather_condition: condições climáticas categóricas (chuva, nublado, limpo, etc.).


**Insights obtidos pelos gráficos de barras:**

- Certos locais de origem e destino concentram mais corridas, indicando pontos estratégicos de alta demanda.

- O clima chuva/neve tende a aumentar a procura por transporte, o que pode se refletir no surge_multiplier.
"""

df_sample = df_part.sample(50000, random_state=42)

numeric_cols = [
    'price', 'distance', 'duration', "surge_multiplier",
    'apparentTemperature', 'precipIntensity', 'hour', "duration",
    "source_encoded", "destination_encoded", "cab_type_encoded",
    "name_encoded", "short_summary_encoded", "icon_encoded"
]
n_cols = 3
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    ax = axes[i]
    sns.histplot(df_sample[col].dropna(), bins=50, kde=True, color='skyblue', ax=ax)
    ax.set_title(f"Distribuição de {col}", fontsize=12)
    ax.set_xlabel(col, fontsize=10)
    ax.set_ylabel('Frequência', fontsize=10)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## 3.5. Interpretação dos Histogramas

A inspeção visual dos histogramas das *features* mais críticas confirma as observações estatísticas (Seção 2.4) e orienta as transformações necessárias para preparar os dados para a modelagem.

### 3.5.1. Variável Alvo e Assimetria

* **`price`:** O histograma da variável alvo demonstra uma **assimetria acentuada à direita** (cauda longa). A maioria das corridas se concentra em preços baixos, mas a presença de **outliers** (corridas caras, até \$89) distorce a distribuição.

### 3.5.2. Preditoras de Viagem

* **`distance` e `duration`:** Ambas as distribuições são **altamente concentradas perto de zero** e apresentam a mesma cauda longa e assimetria à direita. A maioria das viagens é de curta distância.

### 3.5.3. Sazonalidade e Clima

* **`hour` (Hora do Dia):** O gráfico revela uma distribuição **bimodal**, com dois picos de frequência claros:
    * Um pico menor pela manhã (aproximadamente 8h-9h).
    * Um pico maior e mais proeminente no final da tarde (aproximadamente 17h-20h).
    * **Implicação:** Isso confirma os períodos de **pico de demanda (*rush hours*)**, um preditor crítico para a ativação do `surge_multiplier`.

***

### 3.5.4 Resumo dos Padrões

`Distribuições assimétricas`

→ Preço e distância apresentam cauda longa — poucos casos muito altos.

→ Surge_multiplier é pontual, mas eleva drasticamente os preços.

`Picos temporais marcados`

→ Horários de rush (7–9h e 16–19h) concentram a maioria das chamadas.

→ Finais de semana mostram uso mais variado (lazer).

`Fatores ambientais`

→ Clima adverso (chuva, baixa visibilidade) está relacionado a preços mais altos e possíveis atrasos.

→ Temperaturas extremas parecem aumentar o uso do serviço.

`Tipos de corrida`
→ Corridas curtas e locais centrais são mais comuns.
"""

# A distribuição de corridas por hora durante os meses
df_filtro = df[df['cab_type'] != 'Lyft']
plt.figure(figsize=(10, 5))

sns.countplot(
    x=df_filtro['hour'],
    data=df_filtro,
    hue=df_filtro['hour'],
    palette='bright',
    legend=False
)

plt.title('Dsitribuicao de corridas por horas')
plt.show()

"""### Análise: Distribuição de Corridas por Horas

O gráfico de barras confirma o padrão de **demanda bimodal** (dois picos) e a forte correlação entre a hora e o volume de corridas:

* **Picos de Demanda Noturna/Madrugada:** As horas **0h, 1h e 23h** demonstram a maior contagem absoluta de corridas. Isso sugere que o preço é mais volátil e sensível ao *surge* fora do horário comercial (principalmente nos finais de semana).
* **Demanda Diurna:** Há uma alta demanda constante e estável durante a tarde (10h às 17h), representando as horas de pico (*rush hours*).
"""

# A distribuição por tipo de serviço
uber_cab_type = df[df['cab_type'] == 'Uber']['cab_type']
plt.figure(figsize=(10, 5))
sns.countplot(x=uber_cab_type, hue=df['name'], palette='bright')

plt.title('Distribuicao por tipo de serviço Uber')
plt.show()

"""### Análise: Distribuição de Corridas por Tipo de Serviço (Apenas Uber)

Este gráfico revela a concentração de dados por tipo de serviço, o que impacta diretamente a precisão do modelo:

* **Alto Volume:** As corridas **UberX** e **UberXL** dominam a amostra, garantindo que o modelo tenha alta confiança na previsão de preços para estes serviços.
* **Baixo Volume:** Os serviços **Black**, **Black SUV** e **WAV** (acessível) representam uma fração muito menor.

**Implicação:** O modelo de previsão terá maior **incerteza (erro)** nos preços dos serviços *premium* (Black, Black SUV), devido à escassez de dados de treino para estas categorias.
"""

plt.figure(figsize=(10, 5))
uber_cab_type = df[df['cab_type'] == 'Uber']['cab_type']
# Distribuição da duração da viagem do Uber
sns.histplot(data=df, x='duration', hue=uber_cab_type, bins=30, kde=True, palette='bright', multiple='stack')

# Titulo
plt.title('Distribuição por tempo de duração')
plt.xlabel('Duração (minutos)')
plt.ylabel('Frequencia')
plt.show()

"""### Análise: Distribuição de Duração por Tipo de Táxi

O histograma empilhado revela como o tipo de táxi (`cab_type`) se distribui em relação à duração da viagem.

* **Pico em Viagens Curtas:** A concentração de corridas é massiva no eixo Y (Frequência) e no eixo X (Duração) próximo a zero (0-3 minutos). Isso confirma que a grande maioria das viagens, independentemente do tipo de carro, é de **duração muito curta** (e, portanto, curta distância).
* **Dominância do Uber:** A categoria **Uber** domina claramente a frequência em **todos os intervalos de duração**, especialmente no pico de 0-3 minutos, onde o `cab_type='Lyft'` é quase imperceptível.
"""

# Correlação entre o preço e a distância
uber_cab_type = df[df['cab_type'] == 'Uber']['cab_type']
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df, x='distance', y='price', hue=uber_cab_type, palette='bright', alpha=0.7)
plt.title('Preço x Distância')
plt.xlabel('Distancia')
plt.ylabel('Preço')
plt.show()

"""### Análise: Correlação entre Preço e Distância

O gráfico de dispersão revela a relação fundamental entre a distância percorrida (`distance`) e o preço (`price`), separada por plataforma (`cab_type`).

* **Correlação Positiva:** Existe uma **correlação linear positiva clara** entre as variáveis. O preço aumenta à medida que a distância aumenta, o que valida a lógica tarifária básica.
* **Alta Variação em Curta Distância:** Nota-se uma **grande dispersão vertical** (alto ruído de preço) nas distâncias curtas (0 a 3 milhas). Essa variação é causada principalmente pelo tipo de serviço (UberX, Black, etc.) e pela ativação do **`surge_multiplier`**.
* **Dominância da Uber:** A plataforma **Uber** (em azul) abrange a maioria dos pontos de preço mais alto.

**Implicação:** A `distance` é o **preditor primário**, mas o modelo precisará da ajuda de outras *features* (serviço e *surge*) para prever o preço com precisão nas faixas de distâncias curtas, onde a concorrência e a demanda causam maior volatilidade.
"""

# --- MATRIZ DE CORRELAÇÃO ---

COLUNAS_PARA_CORRELACAO = [
    'price', 'distance', 'duration', 'surge_multiplier',
    'apparentTemperature', 'source_encoded', 'destination_encoded',
    'cab_type_encoded', 'name_encoded',
    'short_summary_encoded', 'icon_encoded'
]

numeric_df = df[COLUNAS_PARA_CORRELACAO]
corr_matrix = numeric_df.corr()

plt.figure(figsize=(14, 12))
sns.heatmap(corr_matrix,
            annot=True,
            cmap="coolwarm",
            fmt=".2f",
            linewidths=0.5,
            linecolor='black')
plt.title("Matriz de Correlação", fontsize=16)
plt.show()

"""#Preço x Atributos

`price` tem correlação moderada positiva com:

`distance` (0.35) → faz sentido, corridas mais longas tendem a custar mais.

`duration` (0.35) → corridas mais demoradas também aumentam o preço.

`surge_multiplier` (0.24) → multiplicadores de tarifa influenciam o valor.

price tem correlação negativa forte com name_encoded (-0.58), possivelmente porque certos tipos de Uber (ex.: UberX, Select, Black) têm preços diferentes — e o encoding usado refletiu isso.

#Relevância para o modelo de predição de preço

Mais importantes: `distance`, `duration`, `surge_multiplier`, `name_encoded` (tipo de Uber).

# Parte 2 - Pré-processamento

## 1. Remoção do Lyft

Com o objetivo de criar um modelo de previsão de preço **exclusivo para a plataforma Uber**, a próxima célula realiza a limpeza do *dataset*:

1.  **Identifica** os índices de todas as corridas onde `cab_type` é 'Lyft'.
2.  **Remove** permanentemente essas linhas do DataFrame (`df`) usando o método `.drop()`.

**Implicação:** O DataFrame agora contém apenas corridas da Uber, o que **simplifica a modelagem** e elimina o ruído de preços da concorrência, permitindo que o modelo seja treinado com dados homogêneos.
"""

indices_para_apagar = df[df['cab_type'] == 'Lyft'].index
df.drop(indices_para_apagar, inplace=True)

"""### 2. Codificação de Variáveis Categóricas (One-Hot Encoding)

Com a decisão de evitar o erro ordinal do Label Encoding, esta etapa implementa o **One-Hot Encoding (OHE)**, o método ideal para codificar variáveis **nominais** (que não têm ordem) como tipos de serviço, locais e clima.

A função `pd.get_dummies()` realiza as seguintes transformações:

1.  **Transformação:** Para cada categoria única nas colunas selecionadas (`cab_type`, `name`, `source`, `destination`, `short_summary`), uma **nova coluna binária (0 ou 1)** é criada.
2.  **Multicolinearidade:** O argumento `drop_first=True` remove a primeira categoria de cada grupo. Isso é um polimento crucial que **previne a multicolinearidade**, pois a categoria removida é implicitamente codificada quando todas as outras categorias daquele grupo são 0.

**Resultado:** O DataFrame (`df_encoded_final`) agora é totalmente numérico, pronto para as transformações finais de *Feature Scaling*.
"""

CATEGORICAL_COLS_FINAL = [
    'cab_type',
    'name',
    'short_summary',
    'source',
    'destination'
]

# 1. APLICAÇÃO DO ONE-HOT ENCODING (OHE)
# Transforma as colunas categóricas em múltiplas colunas binárias (0 ou 1)
df_encoded_final = pd.get_dummies(
    df, columns=CATEGORICAL_COLS_FINAL, drop_first=True, dtype=int
)

"""### 3. Divisão Final do Conjunto de Dados (Definição de X e y)

Com o **One-Hot Encoding (OHE)** aplicado (Seção 2 da parte 2), o *dataset* está totalmente numérico. Esta etapa finaliza a preparação dos dados para o modelo:

* **Separação de X e y:** O alvo (`price`) é isolado no conjunto **y**, enquanto o conjunto de preditores **X** é definido, excluindo as colunas redundantes ou substituídas (como as antigas codificações e os valores geográficos originais).
* **Divisão Reprodutível:** A função `train_test_split` é usada para dividir **X** e **y** em conjuntos de **Treino (80%)** e **Teste (20%)**. O `random_state=42` garante que a divisão seja **reprodutível** em qualquer execução.

**Resultado:** Os conjuntos **`X_train_final`**, **`X_test_final`**, **`y_train_final`** e **`y_test_final`** estão limpos e prontos para a fase de **Modelagem**.
"""

from sklearn.model_selection import train_test_split

# Colunas que DEVEM ser removidas antes de treinar:
COLUMNS_TO_DROP_FINAL = [
    'price','datetime',
    'source_encoded', 'destination_encoded', 'cab_type_encoded',
    'name_encoded', 'short_summary_encoded', 'icon_encoded', 'icon', "apparentTemperature",
    "precipIntensity", "precipProbability", "latitude", "longitude"
    ]

# Definindo X (excluindo o target e as colunas problemáticas/redundantes)
X = df_encoded_final.drop(columns=COLUMNS_TO_DROP_FINAL, errors='ignore')
y = df_encoded_final['price']

# 3. REFAZENDO O SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X.info()

"""## Estrutura Final do Conjunto de Preditoras (X)

O conjunto **X** representa o conjunto de *features* final, limpo, transformado e pronto para o treinamento dos modelos.

| Detalhe | Valor | Implicação |
| :--- | :--- | :--- |
| **Total de Registros** | **330.568** | Confirma que todas as linhas nulas de `price` e Lyft foram removidas, resultando em um conjunto de dados limpo e consistente. |
| **Total de Colunas** | **41** | O número de colunas foi drasticamente reduzido das 57 originais, com a substituição das variáveis categóricas pelo **One-Hot Encoding**, eliminando redundâncias e complexidade. |

#### Distribuição das Features

O conjunto **X** é composto por três grupos principais de *features*, todas numéricas:

1.  **Variáveis Contínuas/Transformadas (3):**
    * `distance`, `duration`, e `surge_multiplier`.
2.  **Variáveis Temporais (3):**
    * `hour`, `day`, `month`.
3.  **Variáveis Categóricas (One-Hot Encoded - 35):**
    * As 35 colunas binárias (`0` ou `1`) criadas pelo **One-Hot Encoding** (OHE), representando Serviço, Clima e Localização de forma segura.

#Parte 3 - Hiperparâmetros
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, RidgeCV
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, StackingRegressor,
    HistGradientBoostingRegressor
)

"""### 1. Modelo Baseline: Regressão Linear

O modelo de **Regressão Linear** é definido como a nossa **Linha de Base**. Seu desempenho estabelece a referência mínima de erro que todos os modelos mais complexos (como Random Forest ou HistGradientBoosting) deverão superar após o *tuning*.

* **MAE (Mean Absolute Error):** Representa o erro médio absoluto, o quão distante em dólares a previsão está do preço real. É a métrica mais interpretável.
* **RMSE (Root Mean Squared Error):** Pesa erros maiores mais severamente. É a métrica padrão para a **otimização** e a meta principal que todos os modelos ajustados devem minimizar.

**Conclusão:** Os resultados do Linear Regression estabelecem o **piso de erro**. Os modelos complexos otimizados via *tuning* devem ter um RMSE significativamente menor para serem considerados viáveis no projeto.
"""

#1. TREINAMENTO DO MODELO BASELINE (Linear Regression)
model_baseline = LinearRegression()
model_baseline.fit(X_train, y_train)

y_pred_LR = model_baseline.predict(X_test)


mae = mean_absolute_error(y_test, y_pred_LR)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_LR))

print(f"\nResultados do Linear Regression:")
print(f"MAE (Mean Absolute Error): {mae:.4f}")
print(f"RMSE (Root Mean Squared Error): {rmse:.4f}")

"""## 2. Cross Validation (Validação Cruzada)

Para avaliar de forma mais robusta o desempenho dos modelos, utilizamos a técnica de **Cross Validation**.  

Especificamente, adotamos o método **K-Fold (k=3)**, no qual o conjunto de treino é dividido em 3 partes.  Em cada iteração, 2 partes são usadas para treinar o modelo e a parte restante é usada para validação.  

Esse processo é repetido até que todas as partes tenham servido como conjunto de validação.  Ao final, calculamos a média das métricas obtidas em cada iteração, garantindo uma estimativa mais estável da capacidade de generalização do modelo.  

No projeto, esse valor médio aparece como a métrica **RMSE_CV**, usada para comparar a performance dos diferentes algoritmos testados.

"""

RANDOM_STATE = 42
cv = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)
scoring = "neg_root_mean_squared_error"

"""### Modelos e Espaços de Busca de Hiperparâmetros

Nesta etapa, definimos diferentes modelos de regressão e seus respectivos espaços de busca para tuning com **RandomizedSearchCV**.  
"""

# RandomForest
pipe_rf = RandomForestRegressor(random_state=RANDOM_STATE, oob_score=True)
param_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [10, 15, None],
    "min_samples_split": [2, 5],
    "min_samples_leaf": [1, 2],
    "max_features": ["sqrt"]
}

# SVR
pipe_svr = Pipeline([("scaler", StandardScaler()), ("svr", SVR())])
param_svr = {
    "svr__C": np.logspace(-1, 3, 6),
    "svr__gamma": np.logspace(-4, 0, 5),
    "svr__epsilon": np.logspace(-3, 0, 4),
}

# AdaBoost
pipe_ada = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=2),random_state=RANDOM_STATE)
param_ada = {
    "n_estimators": [50, 100, 200],
    "learning_rate": np.logspace(-3, 0, 5),
    "loss": ["linear", "square", "exponential"],
}

# HistGradientBoosting
pipe_hgb = HistGradientBoostingRegressor(random_state=RANDOM_STATE, early_stopping=True)
param_hgb = {
    "max_iter": [200, 400, 800],
    "learning_rate": np.logspace(-3, 0, 5),
    "max_depth": [3, 5, None],
    "max_leaf_nodes": [31, 63, 127],
    "min_samples_leaf": [20, 50, 100],
    "l2_regularization": [0.0, 0.1, 1.0],
}

# Bagging
pipe_bag = BaggingRegressor(
    estimator=DecisionTreeRegressor(random_state=RANDOM_STATE),
    random_state=RANDOM_STATE,
    oob_score=True
)
param_bag = {
    "n_estimators": [100, 200, 400],
    "estimator__max_depth": [5, 10, None],
    "estimator__min_samples_split": [2, 5],
    "estimator__min_samples_leaf": [1, 2],
    "max_samples": [0.5, 0.7, 1.0],
    "max_features": [0.5, 0.7, 1.0]
}

"""## 3.0 Tuning
A função tune foi criada para padronizar o processo de ajuste de hiperparâmetros dos modelos.
Ela utiliza o RandomizedSearchCV, que ao invés de testar todas as combinações possíveis, realiza uma busca aleatória dentro do espaço de parâmetros definido. Isso torna o processo de tuning muito mais eficiente em tempo e memória, especialmente em datasets grandes, como o de corridas Uber.

Durante os testes, identificamos uma limitação prática: o ambiente de execução não dispunha de memória RAM suficiente para rodar todos os ajustes com 100% dos dados em cada iteração.
Para contornar isso, utilizamos o parâmetro subsample (< 1.0) em alguns modelos.

O parâmetro subsample faz com que, a cada árvore adicionada ao ensemble, apenas uma fração aleatória das amostras de treino seja utilizada. Isso traz três efeitos positivos:

- Reduz consumo de memória e tempo de execução, já que nem todas as amostras são usadas em cada passo.

- Ajuda a evitar overfitting, introduzindo variabilidade estocástica no processo de aprendizado.

- Melhora a generalização, pois o modelo não fica tão dependente de casos específicos.

Na prática, esse ajuste foi fundamental para que conseguíssemos rodar os experimentos no ambiente disponível, garantindo eficiência computacional sem perda significativa de desempenho.
"""

def tune(model, param_dist, name, n_iter=5, subsample=False):
    if subsample:
        # Subamostra menor para caber na RAM
        X_sub = X_train.sample(20000, random_state=RANDOM_STATE)
        y_sub = y_train.loc[X_sub.index]
        data_X, data_y = X_sub, y_sub
        n_jobs_val = 1  # sem paralelismo
    else:
        data_X, data_y = X_train, y_train
        n_jobs_val = -1  # usar múltiplos núcleos

    search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_dist,
        n_iter=n_iter,
        scoring=scoring,
        cv=cv,
        n_jobs=n_jobs_val,
        random_state=RANDOM_STATE,
        verbose=1
    )
    search.fit(data_X, data_y)
    print(f"\n[{name}] Melhor RMSE (CV): {-search.best_score_:.4f}")
    print(f"[{name}] Melhores parâmetros: {search.best_params_}")
    return search.best_estimator_, -search.best_score_

"""#Parte 4 - Modelagem

## Definição da Linha de Base (Baseline)

Antes de iniciar o *tuning* e a comparação de modelos complexos, definimos um modelo de **Regressão Linear** como a nossa *baseline*. O desempenho deste modelo simples é a referência mínima que os algoritmos mais avançados terão que superar.
"""
if __name__ =="__main__":
    lr = Pipeline([("scaler", StandardScaler()), ("lr", LinearRegression())])
    lr.fit(X_train, y_train)
    rmse_lr = -np.mean(cross_val_score(lr, X_train, y_train, cv=cv, scoring=scoring))

    """# Random Forest
    O Random Forest foi utilizado por sua capacidade de capturar relações não lineares e reduzir overfitting pela combinação de múltiplas árvores de decisão. Após o ajuste de hiperparâmetros, o modelo apresentou melhoria em relação ao baseline, reduzindo o RMSE. Isso mostra que a variação no preço das corridas é melhor explicada por padrões não lineares e interações entre variáveis.
    """

    best_rf, rmse_rf = tune(pipe_rf, param_rf, "RandomForest", n_iter=5, subsample=True)

    """# Support Vector Regressor (SVR)
    O Support Vector Regressor (SVR) foi escolhido por sua habilidade em lidar com dados de alta dimensionalidade e buscar margens de erro minimizadas. O uso do kernel RBF possibilitou capturar padrões complexos. O resultado superou o baseline, mas com custo computacional maior, especialmente no tuning.
    """

    best_svr, rmse_svr = tune(pipe_svr, param_svr, "SVR", n_iter=5, subsample=True)

    """# AdaBoost
    O AdaBoost combina diversas árvores fracas e ajusta pesos para focar em exemplos mais difíceis. Apesar disso, o modelo pode ser sensível a outliers, o que se refletiu em um desempenho inferior ao Random Forest e SVR.
    """

    best_ada, rmse_ada = tune(pipe_ada, param_ada, "AdaBoost", n_iter=5)

    """# HistGradientBoosting
    O HistGradientBoosting é uma versão otimizada do Gradient Boosting que utiliza histogramas para acelerar o processo de divisão de nós e reduzir o custo computacional. Essa técnica é especialmente eficiente em datasets grandes, pois agrupa valores contínuos em bins antes de construir as árvores. A presença de early_stopping ajudou a evitar overfitting, interrompendo o treino quando não havia mais ganhos significativos.
    """

    best_hgb, rmse_hgb = tune(pipe_hgb, param_hgb, "HistGradientBoosting", n_iter=5)

    """# Bagging Regressor
    O Bagging Regressor aplica o princípio de "Bootstrap Aggregating", treinando múltiplos modelos de base (geralmente árvores de decisão) em subconjuntos diferentes do dataset, com amostragem com reposição. As previsões finais são a média dos modelos individuais.

    Essa abordagem reduz variância e melhora a estabilidade das previsões em relação a um único modelo de árvore. Nos resultados, o Bagging apresentou métricas competitivas, ficando entre o SVR e o HistGradientBoosting.

    Uma vantagem do Bagging é a possibilidade de usar a métrica Out-of-Bag (OOB) como forma de validação interna, o que dispensa a necessidade de um conjunto de validação separado em alguns casos.
    """

    best_bag, rmse_bag = tune(pipe_bag, param_bag, "Bagging", n_iter=5, subsample= True)

    """# Stacking
    O Stacking combina as previsões de todos os modelos (Random Forest, SVR, AdaBoost e Gradient Boosting) em um meta-modelo de regressão regularizado (RidgeCV). Isso permite unir as forças de cada modelo base.
    """

    meta = RidgeCV(alphas=np.logspace(-4, 4, 20), cv=cv)
    stacking = StackingRegressor(
        estimators=[
            ("lr", lr),
            ("hgb", best_hgb),
            ("bag", best_bag),
        ],
        final_estimator=meta,
        passthrough=False,
        cv=cv,
        n_jobs=1
    )
    stacking.fit(X_train, y_train)

    """### Funções de Avaliação (Métricas Finais)

    Esta seção define as funções utilitárias que garantem que todos os modelos sejam avaliados de forma consistente, usando as métricas mais relevantes para regressão.

    #### 1. Cálculo do RMSE (`calc_rmse`)

    * **Propósito:** Esta função calcula o **RMSE** (Root Mean Squared Error), a métrica principal de **otimização** do projeto.

    #### 2. Função de Avaliação do Modelo (`eval_model`)

    Esta é a função central que usa o conjunto de **Teste** (`X_test`, `y_test`) para calcular as métricas de desempenho final do modelo.

    | Métrica Calculada | Propósito |
    | :--- | :--- |
    | **`RMSE_test`** | O erro quadrático médio no conjunto de **teste** (o erro primário de otimização). |
    | **`MAE_test`** | O **Erro Absoluto Médio** no conjunto de **teste**. É o erro mais fácil de interpretar (ex: "em média, o modelo erra em \$X.XX"). |
    | **`R2_test`** | O **R² (Coeficiente de Determinação)**, que mede a porcentagem da variância do preço explicada pelo modelo. |
    | **`OOB_R2`** | Calculado para modelos *ensemble* (como Random Forest). É a pontuação de **Out-of-Bag** que serve como uma **validação interna** durante o treinamento. |

    **Resultado:** O `eval_model` padroniza o cálculo das métricas de desempenho final, essencial para a **tabela de comparação** de modelos.
    """

    def calc_rmse(y_true, y_pred):
        return np.sqrt(mean_squared_error(y_true, y_pred))

    def eval_model(name, model, oob=False):
        y_pred = model.predict(X_test)
        res = {
            "RMSE_test": calc_rmse(y_test, y_pred),
            "MAE_test": mean_absolute_error(y_test, y_pred),
            "R2_test": r2_score(y_test, y_pred)
        }
        if oob and hasattr(model, "oob_score_"):
            res["OOB_R2"] = model.oob_score_
        return res

    """#  Comparação Final

    | Modelo              | RMSE_CV | RMSE_test | MAE_test | R2_test | OOB_R2 |
    |----------------------|---------|-----------|----------|---------|--------|
    | LinearRegression     | 2.40449 | 2.39438   | 1.63768  | 0.92080 | NaN    |
    | RandomForest         | 2.23126 | 2.17436   | 1.47022  | 0.93469 | 0.9350 |
    | SVR                  | 2.09522 | 2.02919   | 1.20485  | 0.94312 | NaN    |
    | AdaBoost             | 4.33474 | 4.32737   | 3.36224  | 0.74130 | NaN    |
    | HistGradientBoosting | 1.86786 | 1.84829   | 1.13896  | 0.95281 | NaN    |
    | Bagging              | 1.95857 | 1.94735   | 1.20551  | 0.94761 | 0.94935|
    | Stacking             | NaN     | 1.84861   | 1.13794  | 0.95279 | NaN    |

    # Conclusão

    A análise comparativa entre os modelos deixa claro que o baseline (Regressão Linear) foi superado por praticamente todas as técnicas de ensemble ou de aprendizado mais sofisticado.

    Os principais resultados foram:

    - Linear Regression (Baseline) → serviu como referência inicial, com RMSE ≈ 2.39 e R² ≈ 0.92.

    - Random Forest → apresentou melhora consistente, com RMSE ≈ 2.17 e R² ≈ 0.93, além de validação via OOB_R² = 0.935, o que confirma sua capacidade de generalização.

    - SVR → obteve resultados ainda melhores que o Random Forest, com RMSE ≈ 2.02 e R² ≈ 0.94, mostrando a força do kernel RBF para capturar relações não lineares.

    - AdaBoost → foi o modelo com pior desempenho, com RMSE ≈ 4.32 e R² de apenas 0.74, evidenciando sua sensibilidade a outliers.

    - HistGradientBoosting → foi o modelo com melhor desempenho geral, atingindo RMSE ≈ 1.85 e R² ≈ 0.95, além de MAE reduzido. Isso mostra que a combinação de boosting sequencial com histogramas e early stopping proporcionou alta precisão e eficiência.

    - Bagging → teve desempenho sólido (RMSE ≈ 1.94, R² ≈ 0.95), confirmando a estabilidade do método, mas ligeiramente inferior ao HistGradientBoosting.

    - Stacking (Blender) → também alcançou métricas muito próximas ao HistGradientBoosting (RMSE ≈ 1.84 e R² ≈ 0.95), mas não superou de forma consistente o melhor modelo individual.


    Dessa forma, concluímos que o HistGradientBoosting é o modelo escolhido para representar os resultados finais, por oferecer o menor erro (RMSE e MAE) e o maior poder de explicação (R²), superando todos os demais de forma clara e robusta.
    """